{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Image Summarization with BLIP Model**\n",
        "\n",
        "This Python script leverages the BLIP (Bootstrapped Language-Image Pretraining) model from Salesforce to generate summaries about images. By utilizing advanced deep learning techniques, this tool offers a simple way to convert visual data into meaningful text, enhancing accessibility and understanding of image content.\n",
        "\n",
        "## **Overview of the Code**\n",
        "The code is structured into several key components, each responsible for specific tasks, from loading and processing the image to generating and displaying the resulting description.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QJwD8OsIU5Vq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Required Libraries**\n",
        "1. PyTorch: For running the deep learning models on either GPU or CPU.\n",
        "2. Transformers: For loading and using the BLIP model.\n",
        "3. Pillow (PIL): For image processing.\n",
        "4. Matplotlib: For displaying the image and the caption.\n",
        "5. python-multipart\n"
      ],
      "metadata": {
        "id": "ovl8ouUDVn6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision transformers pillow\n",
        "!pip install python-multipart"
      ],
      "metadata": {
        "id": "C6gOpRuQmlBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Key Components**\n",
        "1. Importing Libraries: The script begins by importing essential libraries:\n",
        "\n",
        "  * **torch:** The core library for tensor computation and deep learning operations.\n",
        "  * **PIL:** The Python Imaging Library, used for opening and manipulating image files.\n",
        "  * **matplotlib.pyplot:** A plotting library to visualize images and their captions.\n",
        "  * **transformers:** A library from Hugging Face that provides pre-trained models and processing utilities for natural language processing (NLP) and image captioning and summarization tasks.\n"
      ],
      "metadata": {
        "id": "lZNI8k-Kp6l3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration"
      ],
      "metadata": {
        "id": "j_q6od93oLAL"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Loading the BLIP Model: [link text](https://)The script initializes the BLIP model and its corresponding processor:\n",
        "\n",
        "  * The BlipProcessor handles the image preprocessing and input formatting required by the model.\n",
        "  * The BlipForConditionalGeneration is the actual model used for generating captions based on the processed images.\n",
        "  * The model is moved to the GPU (if available) for faster computation.\n",
        "\n"
      ],
      "metadata": {
        "id": "AbHOrSuvVO6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "ZiAePqfmoewM"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Generating Image Descriptions: The function generate_image_description is responsible for loading an image, processing it, and generating a caption:\n",
        "\n",
        "  * The image is loaded and converted to RGB format to ensure compatibility with the model.\n",
        "  *The image is then processed into a format suitable for the BLIP model.\n",
        "  *The model generates a caption with a maximum length of 150 tokens, utilizing beam search for improved quality and coherence of the generated text.\n",
        "  *The generated output is decoded into a human-readable format."
      ],
      "metadata": {
        "id": "svCTVHq4ViJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_image_description(image_path):\n",
        "    \"\"\"\n",
        "    Generate a detailed description for the image using the BLIP model.\n",
        "    \"\"\"\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Prepare the inputs for BLIP\n",
        "    inputs = blip_processor(images=image, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate the description\n",
        "    output = blip_model.generate(**inputs, max_length=150, num_beams=5, early_stopping=True)\n",
        "    description = blip_processor.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    return image, description\n"
      ],
      "metadata": {
        "id": "kWeX0F1tpFGc"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Displaying the Image and Description: The function display_image_and_description is used to visualize the image along with its generated caption\n",
        "\n",
        "  * A Matplotlib figure is created, and the image is displayed without axes for a cleaner look.\n",
        "  * The generated description is set as the title of the image, allowing users to quickly understand the content.\n"
      ],
      "metadata": {
        "id": "hs6oKkO4pJNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_image_and_description(image, description):\n",
        "    \"\"\"\n",
        "    Display the image and its generated description.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')  # Hide the axes\n",
        "    plt.title(description, fontsize=14)  # Show the description as the title\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "s5YcvlfDpIDl"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Example Usage: The script concludes with an example of how to use the functions defined\n",
        "\n",
        "  * The user is expected to replace the image_path variable with the path to their image file.\n",
        "  * The image and its description are generated and displayed when the script is executed.\n"
      ],
      "metadata": {
        "id": "yf-8xpJvpjHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "image_path = \"your_model_path.jpg\"  # Replace with the path to your image\n",
        "image, description = generate_image_description(image_path)\n",
        "display_image_and_description(image, description)\n"
      ],
      "metadata": {
        "id": "qlbLoFlrpYCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**\n",
        "This project demonstrates the application of advanced AI models for image captioning using the BLIP (Bootstrapped Language-Image Pretraining) model. By utilizing this model, we are able to generate meaningful, detailed descriptions of images, transforming visual data into human-readable text. This has various applications, listed below.\n",
        "\n",
        "The project also incorporates Python libraries such as PyTorch for running deep learning models, Transformers for leveraging pre-trained models, and PIL and Matplotlib for image handling and visualization. The dynamic caption generation system is flexible and can be applied to a wide range of images, making it a robust tool for image understanding tasks.\n",
        "\n",
        "With further improvements, such as integrating it into web applications or expanding the model's scope, this project can serve as a valuable foundation for AI-powered image processing and natural language generation applications.\n",
        "\n",
        "\n",
        "Few example usage of this project can be:\n",
        "1. Photo Archiving and Tagging Systems\n",
        "2. Automatic Captioning for News and Media Outlets\n",
        "3. Content Moderation with Descriptive Summaries of Uploaded Images\n",
        "4. Interactive Storytelling Applications Using AI\n",
        "5. AI-powered Presentation Tools for Auto-generating Slide Descriptions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZEZJWO0LqwHJ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}